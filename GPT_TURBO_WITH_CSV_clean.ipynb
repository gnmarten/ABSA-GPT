{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter your csv filename here\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Fintro_Twitter_2016.csv', delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the prompt here\n",
    "completion_text_conversation = \"\"\"\n",
    "Please perform quadruple sentiment extraction on this utterance by extracting \\\"Aspect Category\\\", \\\"Aspect Term\\\", \\\"Opinion Target\\\", \\\"Opinion Expression\\\", \\\"Sentiment\\\" and \\\"Confidence Score\\\" using the original language of the text. The aspect categories and their terms are: \\\"TEXT: Title\\\", \\\"TEXT: Quote\\\", \\\"TEXT: PoV_Narration\\\", \\\"TEXT: Motifs_Themes\\\", \\\"TEXT: Language_Style_\\\", \\\"TEXT: General-Content_Plot\\\", \\\"TEXT: General\\\", \\\"TEXT: Form\\\", \\\"TEXT: Flow_Rhythm_Punctuation\\\", \\\"TEXT: Cover_Edition\\\", \\\"TEXT: Characters\\\", \\\"READING: Pronunciation_Intonation_Understandability\\\", \\\"READING: General\\\", \\\"READING: Flow_Rhythm_Punctuation\\\", \\\"ONSITE-AUDIENCE: General\\\", \\\"ONSITE-AUDIENCE: Behaviour\\\", \\\"ONSITE-AUDIENCE: Appearance_Clothing\\\", \\\"ONSITE-AUDIENCE: Age\\\", \\\"META: Winner_Award-Ceremony\\\", \\\"META: Weather\\\", \\\"META: Voting\\\", \\\"META: Videoportrait\\\", \\\"META: Technology_Social-Media\\\", \\\"META: Side-Event\\\", \\\"META: Shortlist\\\", \\\"META: Opening-Speech\\\", \\\"META: Online-Assessment\\\", \\\"META: Music\\\", \\\"META: Montage\\\", \\\"META: Main-Event\\\", \\\"META: Longlist\\\", \\\"META: Location\\\", \\\"META: Literature_Literary-Prizes\\\", \\\"JURY: Voice_Language-Use\\\", \\\"JURY: Quote\\\", \\\"JURY: General\\\", \\\"JURY: Discussion_Valuation\\\", \\\"JURY: Behaviour\\\", \\\"JURY: Appearance_Clothing\\\", \\\"JURY: Age\\\", \\\"CONTENDER: Voice_Language-Use\\\", \\\"CONTENDER: Quote\\\", \\\"CONTENDER: General\\\", \\\"CONTENDER: Gender\\\", \\\"CONTENDER: Appearance_Clothing\\\", \\\"CONTENDER: Age\\\", \\\"ALLO-REFERENCES: TEXT_Other-Text\\\", \\\"ALLO-REFERENCES: TEXT_Other-Author\\\", \\\"ALLO-REFERENCES: SCREEN_Film_Tv\\\", \\\"ALLO-REFERENCES: SCREEN_Director_Actor\\\", \\\"ALLO-REFERENCES: MUSIC_Musician\\\", \\\"ALLO-REFERENCES: MUSIC_Music\\\", \\\"ALLO-REFERENCES: General\\\", \\\"ALLO-REFERENCES: OTHER_Person\\\". Any reference to one of the names Kathleen Cools, Bart Vanegeren, Edith Aerts, Jeroen van Kan, Danny Theuwis, Guinevere Claeys, Adil El Arbi needs to be classified as one of the \\\"JURY\\\" aspect categories. Any reference to either one of the following names needs to be classified as one of the \\\"CONTENDER\\\" aspect categories: Jan Brokken, Saskia De Coster, Chris De Stoop, Stephan Enter, Arjen Fortuin, Philip Huff, Willem Otterspeer, Jamal Ouariachi, Connie Palmen, Hagar Peeters, Yves Petry, Inge Schilperoord, K. Schippers, Pieter Steinz, P.F. Thom√©se, Joke van Leeuwen, Annelies Verbeke, Peter Verhelst, L.H. Wiener, Maartje Wortel. Any other name is to be seen as belonging to the Aspect Category \\\"ALLO-REFERENCE\\\". The Sentiment is  \\\"positive\\\", \\\"neutral\\\" or \\\"negative\\\". The numeric confidence score ranges between -1 and 1 and is rounded to hundreds and tens. The output is a single JSON object that contains a list of JSON objects. The confidence score is represented as a string in the output. The order of the list of JSON objects is the same as the order in which the opinions were expressed in the text. Ignore the hashtags and the weblinks. Reply only with the json. Do not explain. Analyze sentiment for each opinion target separately. All entities mentioned in the text do belong to at least one of the aspect categories and terms.\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import subprocess\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from pprint import pprint\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"yourkey\" #be aware that this will cost, that tokens in some languages are more expensive etc.\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def print_response(response):\n",
    "    if response.status_code == 200:\n",
    "        print(response.json())\n",
    "        return response.json()\n",
    "    else:\n",
    "        error = response.json()\n",
    "        print(f\"Error: {error['error']}\")\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Setup OPENAI_API_KEY\n",
    "\n",
    "# Setup logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(format=\"%(asctime)s | %(levelname)s | %(message)s\", level=logging.INFO)\n",
    "\n",
    "# Update sys.path (or use PYTHONPATH)\n",
    "\n",
    "#sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from tqdm.notebook import tqdm\n",
    "from json import loads\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "\n",
    "analysis_results = []\n",
    "extra_prompts = []\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/completions\", \\\n",
    "    headers=headers, json={\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"prompt\": \"Say this is a test\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "def create_completion(messages:str, suffix:str = None, model: str = \"gpt-3.5-turbo\", max_tokens: Optional[int] = 500, temperature: float = 1, top_p: Optional[int] = 1, n: Optional[int] = 1,  stream: Optional[bool] = False, echo: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    https://platform.openai.com/docs/api-reference/completions/create\n",
    "    ##### Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): \n",
    "            - The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays. \n",
    "            Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. \n",
    "        \n",
    "        - suffix (str): \n",
    "            - The suffix that comes after a completion of inserted text.\n",
    "        \n",
    "        - model (str, required): \n",
    "            - ID of the model to use. You can use the **List models** API to see all of your available models, or see our **Model overview** for descriptions of them. Defaults to \"text-davinci-003\".\n",
    "        \n",
    "        - max_tokens (int, optional, Default: 500): \n",
    "            - The maximum number of tokens to generate in the completion.\n",
    "                The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Defaults to 500.\n",
    "        \n",
    "        - temperature (float, optional): \n",
    "            - What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "            We generally recommend altering this or `top_p` but not both. Defaults to 0.\n",
    "            \n",
    "        - top_p (float, optional):\n",
    "            - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "            - We generally recommend altering this or `temperature` but not both. Defaults to 1.\n",
    "            \n",
    "        - n (int, optional, Default: 1):\n",
    "            - The number of completions to generate. Defaults to 1. \n",
    "        \n",
    "        - stream (bool, optional): \n",
    "            - Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message. Defaults to False.\n",
    "        \n",
    "        - echo (bool, optional): _description_. Defaults to False.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    response = requests.post(\"https://api.openai.com/v1/completions\", \\\n",
    "        headers=headers, json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"suffix\": suffix,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stream\": stream,\n",
    "            \"echo\": echo,\n",
    "            \"stop\": suffix,\n",
    "            \"n\": n,\n",
    "            \"top_p\": top_p\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "    \n",
    "def edit_prompt(instruction: str, input: Optional[str] = None, model: str = \"text-davinci-edit-001\", num: Optional[int] = 1, temperature: Optional[float] = 1, top_p: Optional[float] = 1):\n",
    "    \"\"\"#### Given a prompt and an instruction, the model will return an edited version of the prompt.\n",
    "    https://platform.openai.com/docs/api-reference/edits\n",
    "    Args:\n",
    "        - instruction (str, required): \n",
    "            - The instruction that tells the model how to edit the prompt.\n",
    "            \n",
    "        - input (Optional[str], optional): \n",
    "            - The input text to use as a starting point for the edit. Defaults to None.\n",
    "        \n",
    "        - model (str, required): \n",
    "            - ID of the model to use. You can use the `text-davinci-edit-001` or `code-davinci-edit-001` model with this endpoint. Defaults to \"text-davinci-edit-001\".\n",
    "\n",
    "        - n (Optional[int], optional): \n",
    "            - How many edits to generate for the input and instruction. Defaults to 1.\n",
    "\n",
    "        - temperature (Optional[float], optional): \n",
    "            - What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "            - We generally recommend altering this or `top_p` but not both. Defaults to 1.\n",
    "        \n",
    "        - top_p (Optional[float], optional):\n",
    "            - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "            - We generally recommend altering this or `temperature` but not both. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        - response\n",
    "    \"\"\"    \n",
    "    response = requests.post(\"https://api.openai.com/v1/edits\", \\\n",
    "        headers=headers, json={\n",
    "            \"model\": model,\n",
    "            \"input\": input,\n",
    "            \"instruction\": instruction,\n",
    "            \"n\": num,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Caption': 'text', 'Filename': 'title'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import traceback\n",
    "from textwrap import dedent\n",
    "from random import choice\n",
    "from tqdm.notebook import tqdm\n",
    "#from absa.analysis.gpt3 import analyze\n",
    "from json import loads\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "import time\n",
    "from json import JSONDecodeError, loads\n",
    "from tqdm import tqdm\n",
    "\n",
    "retry_attempts = 3\n",
    "retry_delay = 2\n",
    "analysis_results = []\n",
    "extra_prompts = []\n",
    "\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.INFO)\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Analyzing reviews\"):\n",
    "    try:\n",
    "        if pd.notnull(row[\"title\"]):\n",
    "            title = row[\"title\"]\n",
    "            text = row[\"text\"]\n",
    "            log.info(f\"Analyzing feedback - \\nID: {row['title']}\\nTitle: {title}\\nText: {text}\\n\")\n",
    "\n",
    "            res = create_completion(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=\"0.2\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\",    \"content\": \"You are a data analyst that provides multi-target quadruple sentiment extraction.\"},\n",
    "                    {\"role\": \"system\",    \"content\": \"You annotate multiple opinion targets per line if the text contains more than one target. You annotate every single target only once. If the input text does not seem to contain any opinion targets or expressions, assume as opinion target the first noun and as expression the rest of the utterance. Hashtags and weblinks are not targets. Do not invent aspect categories or aspect terms yourself.\"},\n",
    "                    {\"role\": \"user\",      \"content\": completion_text_conversation + text + \":\"},    \n",
    "                    #{\"role\": \"user\",      \"content\": \"What is the most likely \"},\n",
    "                    #{\"role\": \"user\",      \"content\": \"Where are most of the students from?\"},\n",
    "                    #{\"role\": \"user\",      \"content\": \"Which extra-curricular activity/activities is most popular? Which among them has the highest female participation?\"},\n",
    "                    # add in as many questions as you want\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                raw_json = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                #print(raw_json)\n",
    "                json_data = loads(raw_json)\n",
    "                analysis_results.append(json_data)\n",
    "\n",
    "                log.debug(f\"JSON response: {pprint(json_data)}\")\n",
    "\n",
    "                extra_prompts.append(f\"\\n{text}\\n{raw_json}\")\n",
    "\n",
    "            except JSONDecodeError as e:\n",
    "                if \"Unfortunately, it seems that there is no text provided in your request for me to perform sentiment extraction on.\" in str(e):\n",
    "                    log.error(\"No text provided for sentiment analysis\")\n",
    "                    analysis_results.append(\"No text provided for sentiment analysis\")\n",
    "                elif \"Expecting value\" in str(e):\n",
    "                    if \"Unterminated string\" in str(e):\n",
    "                        log.warning(\"JSON string is unterminated\")\n",
    "                        analysis_results.append(raw_json)\n",
    "                    elif \"I'm sorry, but I need some text to perform the sentiment extraction\" in str(e):\n",
    "                        log.error(\"No text provided for sentiment analysis\")\n",
    "                        analysis_results.append(\"No text provided for sentiment analysis\")\n",
    "                    else:\n",
    "                        log.error(f\"Failed to parse '{raw_json}' -> {e}\")\n",
    "                        analysis_results.append([])\n",
    "                else:\n",
    "                    log.error(f\"Failed to parse '{raw_json}' -> {e}\")\n",
    "                    analysis_results.append([])\n",
    "            except Exception as e:\n",
    "                log.error(f\"Error performing sentiment analysis on text: {text}\")\n",
    "                log.error(str(e))\n",
    "                analysis_results.append([])\n",
    "        else:\n",
    "            log.warning(f\"No id found in row {i}. Skipping analysis.\")\n",
    "            analysis_results.append([])\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        log.warning(f\"The error ecountered, {i}. Skipping analysis.\")\n",
    "        analysis_results.append([])\n",
    "        \n",
    "    #time.sleep(2) #added 2 seconds delay between requests\n",
    "\n",
    "df[\"analysis\"] = analysis_results\n",
    "#df.to_csv(\"./feedbacks_analysis_2015_jury_batch1.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to rename this, or your results file will get overwritten\n",
    "df.to_csv(\"./feedbacks_analysis_2016_fintro.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt: Please perform quadruple sentiment extraction on this utterance by extracting \\\"Aspect Category\\\", \\\"Aspect Term\\\", \\\"Opinion Target\\\", \\\"Opinion Expression\\\", \\\"Sentiment\\\" and \\\"Confidence Score\\\" using the original language of the text. The aspect categories and their terms are: \\\"TEXT: Title\\\", \\\"TEXT: Quote\\\", \\\"TEXT: PoV_Narration\\\", \\\"TEXT: Motifs_Themes\\\", \\\"TEXT: Language_Style_\\\", \\\"TEXT: General-Content_Plot\\\", \\\"TEXT: General\\\", \\\"TEXT: Form\\\", \\\"TEXT: Flow_Rhythm_Punctuation\\\", \\\"TEXT: Cover_Edition\\\", \\\"TEXT: Characters\\\", \\\"READING: Pronunciation_Intonation_Understandability\\\", \\\"READING: General\\\", \\\"READING: Flow_Rhythm_Punctuation\\\", \\\"ONSITE-AUDIENCE: General\\\", \\\"ONSITE-AUDIENCE: Behaviour\\\", \\\"ONSITE-AUDIENCE: Appearance_Clothing\\\", \\\"ONSITE-AUDIENCE: Age\\\", \\\"META: Winner_Award-Ceremony\\\", \\\"META: Weather\\\", \\\"META: Voting\\\", \\\"META: Videoportrait\\\", \\\"META: Technology_Social-Media\\\", \\\"META: Side-Event\\\", \\\"META: Shortlist\\\", \\\"META: Opening-Speech\\\", \\\"META: Online-Assessment\\\", \\\"META: Music\\\", \\\"META: Montage\\\", \\\"META: Main-Event\\\", \\\"META: Longlist\\\", \\\"META: Location\\\", \\\"META: Literature_Literary-Prizes\\\", \\\"JURY: Voice_Language-Use\\\", \\\"JURY: Quote\\\", \\\"JURY: General\\\", \\\"JURY: Discussion_Valuation\\\", \\\"JURY: Behaviour\\\", \\\"JURY: Appearance_Clothing\\\", \\\"JURY: Age\\\", \\\"CONTENDER: Voice_Language-Use\\\", \\\"CONTENDER: Quote\\\", \\\"CONTENDER: General\\\", \\\"CONTENDER: Gender\\\", \\\"CONTENDER: Appearance_Clothing\\\", \\\"CONTENDER: Age\\\", \\\"ALLO-REFERENCES: TEXT_Other-Text\\\", \\\"ALLO-REFERENCES: TEXT_Other-Author\\\", \\\"ALLO-REFERENCES: SCREEN_Film_Tv\\\", \\\"ALLO-REFERENCES: SCREEN_Director_Actor\\\", \\\"ALLO-REFERENCES: MUSIC_Musician\\\", \\\"ALLO-REFERENCES: MUSIC_Music\\\", \\\"ALLO-REFERENCES: General\\\", \\\"ALLO-REFERENCES: OTHER_Person\\\". Any reference to one of the names Kathleen Cools, Bart Vanegeren, Edith Aerts, Jeroen van Kan, Danny Theuwis, Guinevere Claeys, Adil El Arbi needs to be classified as one of the \\\"JURY\\\" aspect categories. Any reference to either one of the following names needs to be classified as one of the \\\"CONTENDER\\\" aspect categories: Jan Brokken, Saskia De Coster, Chris De Stoop, Stephan Enter, Arjen Fortuin, Philip Huff, Willem Otterspeer, Jamal Ouariachi, Connie Palmen, Hagar Peeters, Yves Petry, Inge Schilperoord, K. Schippers, Pieter Steinz, P.F. Thom√©se, Joke van Leeuwen, Annelies Verbeke, Peter Verhelst, L.H. Wiener, Maartje Wortel. Any other name is to be seen as belonging to the Aspect Category \\\"ALLO-REFERENCE\\\". The Sentiment is  \\\"positive\\\", \\\"neutral\\\" or \\\"negative\\\". The numeric confidence score ranges between -1 and 1 and is rounded to hundreds and tens. The output is a single JSON object that contains a list of JSON objects. The confidence score is represented as a string in the output. The order of the list of JSON objects is the same as the order in which the opinions were expressed in the text. Ignore the hashtags and the weblinks. Reply only with the json. Do not explain. Analyze sentiment for each opinion target separately. All entities mentioned in the text do belong to at least one of the aspect categories and terms.\\n\\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
